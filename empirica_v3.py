# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SjdWvLFNghF15WXen6ieH-fVXtU1X55k
"""

# ============================================================================
# EMPIRICA v4.1 ‚Äî PATCH (paste this in a NEW Colab cell after the main code)
# ============================================================================
# Fixes:
#   1. Tautology guard ‚Äî rejects X/Y from same indicator family
#   2. Hardcoded hypothesis ‚Äî no more input() issues in Colab
#   3. Stronger AI parsing prompt ‚Äî forces healthcare spending ‚Üí life expectancy
#   4. Literature search retry with longer timeout
#   5. Title always shows in document
# ============================================================================
!pip install python-docx anthropic pandas statsmodels scipy numpy requests

import os

# --- Your API key (replace if needed) ---
os.environ["ANTHROPIC_API_KEY"] = "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"

# ============================================================================
# FIX 1: Tautology guard
# ============================================================================
# Indicator families ‚Äî if X and Y share a prefix, it's a tautology
INDICATOR_FAMILIES = {
    "NY.GDP": "GDP",
    "SE.XPD": "Education spending",
    "SH.XPD": "Health spending",
    "SP.DYN": "Demographics",
    "SI.POV": "Poverty",
}

def check_tautology(x_code: str, y_code: str) -> bool:
    """Returns True if X and Y are from the same indicator family (tautology)."""
    for prefix in INDICATOR_FAMILIES:
        if x_code.startswith(prefix) and y_code.startswith(prefix):
            return True
    # Also catch exact same indicator
    if x_code == y_code:
        return True
    return False

# ============================================================================
# FIX 2: Override ai_parse_hypothesis with stronger constraints
# ============================================================================
_original_parse = ai_parse_hypothesis  # save original

def ai_parse_hypothesis_fixed(hypothesis_text: str) -> dict:
    """Patched version with tautology check and retry logic."""
    print("\nü§ñ AI is analyzing your hypothesis (v4.1 patched)...")

    plan = ask_claude_json(
        system="""You are a research methodology expert. Given a hypothesis, pick World Bank indicators.

CRITICAL RULES:
1. X and Y MUST be from DIFFERENT domains (e.g., health spending ‚Üí life expectancy, NOT GDP growth ‚Üí GDP per capita growth)
2. NEVER pick two GDP indicators, two health indicators, or two education indicators as X and Y
3. The relationship must be CAUSAL/INTERESTING, not an accounting identity
4. Pick 2-4 control variables that are CONFOUNDERS (affect both X and Y)

World Bank indicators:
ECONOMIC:
- NY.GDP.PCAP.PP.KD = GDP per capita (PPP, constant 2017 $)
- NY.GDP.PCAP.KD.ZG = GDP per capita growth (annual %)
- FP.CPI.TOTL.ZG = Inflation (annual %)
- SL.UEM.TOTL.ZS = Unemployment (%)

EDUCATION:
- SE.XPD.TOTL.GD.ZS = Education expenditure (% of GDP)
- SE.SEC.ENRR = Secondary school enrollment (% gross)
- SE.TER.ENRR = Tertiary enrollment (% gross)

HEALTH:
- SH.XPD.CHEX.GD.ZS = Health expenditure (% of GDP)
- SP.DYN.LE00.IN = Life expectancy (years)
- SP.DYN.IMRT.IN = Infant mortality rate (per 1,000)
- SH.MED.PHYS.ZS = Physicians (per 1,000 people)

INFRASTRUCTURE & GOVERNANCE:
- IT.NET.USER.ZS = Internet users (% of population)
- EG.ELC.ACCS.ZS = Access to electricity (%)
- SP.URB.TOTL.IN.ZS = Urban population (%)
- GE.EST = Government effectiveness

INEQUALITY:
- SI.POV.GINI = Gini index
- SP.POP.GROW = Population growth (%)

Return JSON:
{
    "title": "Academic paper title (specific, not generic)",
    "statement": "Cleaned hypothesis",
    "independent_var": "World Bank indicator code for X (the CAUSE)",
    "dependent_var": "World Bank indicator code for Y (the EFFECT)",
    "x_label": "Human-readable label for X",
    "y_label": "Human-readable label for Y",
    "control_vars": [
        {"code": "indicator code", "label": "label", "rationale": "why"}
    ],
    "start_year": 2000,
    "end_year": 2023,
    "pubmed_query": "search query for PubMed",
    "semantic_scholar_query": "search query for Semantic Scholar",
    "reasoning": "explanation"
}

EXAMPLE for "healthcare spending ‚Üí life expectancy":
- independent_var: "SH.XPD.CHEX.GD.ZS" (health expenditure % GDP)
- dependent_var: "SP.DYN.LE00.IN" (life expectancy)
- controls: GDP per capita, education enrollment, urbanization""",
        user=f'Hypothesis: "{hypothesis_text}"\n\nPick the CORRECT X and Y indicators. X must be the CAUSE, Y must be the EFFECT.',
    )

    # --- TAUTOLOGY CHECK ---
    if check_tautology(plan["independent_var"], plan["dependent_var"]):
        print(f"  ‚ö†Ô∏è  TAUTOLOGY DETECTED: {plan['independent_var']} ‚Üí {plan['dependent_var']}")
        print(f"  ‚ö†Ô∏è  Overriding with correct indicators for this hypothesis...")

        # Fallback: parse the hypothesis for common patterns
        h = hypothesis_text.lower()
        if "health" in h and ("life expectancy" in h or "mortality" in h or "life" in h):
            plan["independent_var"] = "SH.XPD.CHEX.GD.ZS"
            plan["dependent_var"] = "SP.DYN.LE00.IN"
            plan["x_label"] = "Current health expenditure (% of GDP)"
            plan["y_label"] = "Life expectancy at birth (years)"
        elif "education" in h and ("gdp" in h or "growth" in h or "income" in h):
            plan["independent_var"] = "SE.XPD.TOTL.GD.ZS"
            plan["dependent_var"] = "NY.GDP.PCAP.PP.KD"
            plan["x_label"] = "Government expenditure on education (% of GDP)"
            plan["y_label"] = "GDP per capita (PPP, constant 2017 $)"
        elif "internet" in h and ("gdp" in h or "growth" in h or "income" in h):
            plan["independent_var"] = "IT.NET.USER.ZS"
            plan["dependent_var"] = "NY.GDP.PCAP.PP.KD"
            plan["x_label"] = "Individuals using the Internet (% of population)"
            plan["y_label"] = "GDP per capita (PPP, constant 2017 $)"

        print(f"  ‚úÖ Corrected to: {plan['x_label']} ‚Üí {plan['y_label']}")

    # --- CONTROL VARIABLE CHECK ---
    if len(plan.get("control_vars", [])) < 2:
        print(f"  ‚ö†Ô∏è  Only {len(plan.get('control_vars', []))} control(s). Adding defaults...")
        default_controls = [
            {"code": "NY.GDP.PCAP.PP.KD", "label": "GDP per capita (PPP)", "rationale": "Income level confounder"},
            {"code": "SE.SEC.ENRR", "label": "Secondary school enrollment", "rationale": "Education confounder"},
            {"code": "SP.URB.TOTL.IN.ZS", "label": "Urban population (%)", "rationale": "Urbanization confounder"},
        ]
        existing_codes = {c["code"] for c in plan.get("control_vars", [])}
        # Don't add controls that overlap with X or Y
        for dc in default_controls:
            if dc["code"] not in existing_codes and dc["code"] != plan["independent_var"] and dc["code"] != plan["dependent_var"]:
                plan.setdefault("control_vars", []).append(dc)
                if len(plan["control_vars"]) >= 3:
                    break

    print(f"  ‚Üí Title: {plan['title']}")
    print(f"  ‚Üí X: {plan['x_label']} ({plan['independent_var']})")
    print(f"  ‚Üí Y: {plan['y_label']} ({plan['dependent_var']})")
    print(f"  ‚Üí Controls: {', '.join(c['label'] for c in plan['control_vars'])}")
    print(f"  ‚Üí Years: {plan['start_year']}-{plan['end_year']}")

    return plan

# Replace the function
ai_parse_hypothesis = ai_parse_hypothesis_fixed

# ============================================================================
# FIX 3: Literature search with retry and longer timeout
# ============================================================================
_original_ss_search = SemanticScholarSearcher.search

def ss_search_with_retry(self, query, max_results=8):
    """Retry Semantic Scholar with longer timeout."""
    for attempt in range(3):
        try:
            print(f"\nüìñ Semantic Scholar search (attempt {attempt+1}): {query}")
            resp = requests.get(
                f"{self.BASE_URL}/paper/search",
                params={
                    "query": query, "limit": max_results,
                    "fields": "title,authors,year,journal,externalIds,abstract,citationCount",
                },
                timeout=30,  # longer timeout
            )
            resp.raise_for_status()
            papers = resp.json().get("data", [])
            if papers:
                break
        except Exception as e:
            print(f"  ‚Üí Attempt {attempt+1} failed: {e}")
            if attempt < 2:
                time.sleep(2)
            papers = []

    articles = []
    for p in papers:
        try:
            title = p.get("title", "")
            authors_raw = p.get("authors", [])
            authors = [a.get("name", "") for a in authors_raw if a.get("name")]
            year = str(p.get("year", ""))
            if not (title and authors and year and year != "None"):
                continue
            journal_info = p.get("journal")
            journal = journal_info.get("name", "Unknown") if journal_info else "Unknown"
            ext_ids = p.get("externalIds", {}) or {}
            doi = ext_ids.get("DOI", "")
            abstract = (p.get("abstract") or "")[:500]
            citations = p.get("citationCount", 0) or 0
            authors_short = f"{authors[0]} et al." if len(authors) > 3 else ", ".join(authors)
            articles.append({
                "title": title, "authors": authors, "authors_short": authors_short,
                "year": year, "journal": journal, "doi": doi, "pmid": "",
                "abstract": abstract, "citations": citations, "source": "Semantic Scholar",
            })
            print(f"  ‚Üí {authors_short} ({year}) [{citations} cites] - {title[:60]}...")
        except Exception:
            continue
    articles.sort(key=lambda a: a.get("citations", 0), reverse=True)
    return articles

SemanticScholarSearcher.search = ss_search_with_retry

# ============================================================================
# FIX 4: Ensure title always appears in document
# ============================================================================
_original_create = DocumentAssembler.create

def create_with_title_fix(self, plan, sections, all_results, literature, controls_fetched, output_path):
    """Ensure title is never empty."""
    if not plan.get("title") or plan["title"].strip() == "":
        plan["title"] = f"The Effect of {plan['x_label']} on {plan['y_label']}: A Cross-Country Panel Analysis"
    _original_create(self, plan, sections, all_results, literature, controls_fetched, output_path)

DocumentAssembler.create = create_with_title_fix


# ============================================================================
# üöÄ RUN ‚Äî hardcoded, no input() needed
# ============================================================================

print("\n" + "=" * 60)
print("  RUNNING EMPIRICA v4.1 (patched)")
print("=" * 60)

run_empirica("Higher government healthcare spending as a share of GDP leads to longer life expectancy at birth")

from google.colab import files
import os

# Download the paper
files.download('output/paper.docx')

# Download the reproduction code
files.download('output/reproduce.py')